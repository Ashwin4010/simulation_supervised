network: tiny_v2
learning_rate: 0.01
optimizer: gradientdescent
random_seed: 123
max_episodes: 100
real: True
# POLICY
speed: 0.3
action_bound: 0.6
discrete: True
break_and_turn: True
epsilon: 0.
epsilon_decay: 0.
# TRAINING
empty_buffer: True
buffer_size: 100
batch_size: 60
max_gradient_steps: 10
backward_pass_every: batch
# VALIDATE ON OFFLINE DATASET
# validate_offline: True
# dataset: online_corridor_ABC_expert
# load_data_in_ram: True
# num_threads: 1
# INITIALIZATION
# scratch: True
continue_training: True
# load_config: True
# LLL
# lifelonglearning: True
update_importance_weights: True
lll_omage_update: running_average
lll_weight: 1
# loss_window_mean: 1
loss_window_mean: 0.5
# loss_window_var: 0.2
loss_window_var: 0.02
loss_window_length: 5
# loss_peak_threshold: 0.45
# HARD REPLAY BUFFER
hard_replay_buffer: True
hard_batch_size: 30
# NORMALIZE
normalize_over_actions: True
# train_features_only: True